{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBVdnv8gj5Q3AYD2Gtlt9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VDeshmukhCemtrex/Natural-Language-Processing/blob/main/NLP_using_spaCy_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy EntityRuler"
      ],
      "metadata": {
        "id": "cP0q-lz2scAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVlpB0Pjn8Y0",
        "outputId": "c6f2eaae-162b-4832-90bc-3bab976b29ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiL-KsLeoGtE",
        "outputId": "f3c5c9ab-348b-4bfd-bc00-1338d81fa196"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treblinka GPE\n",
            "Poland GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "patterns = [\n",
        "                {\"label\": \"LOC\", \"pattern\": \"Treblinka\"}\n",
        "            ]\n",
        "ruler.add_patterns(patterns)\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqvyAX0AoJnC",
        "outputId": "977462ef-fc40-422b-eee3-1d8cfa12c3a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treblinka GPE\n",
            "Poland GPE\n",
            "Treblinka LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.analyze_pipes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nm4DAO7ovEe",
        "outputId": "ece71193-f6da-4014-ccbc-8a1d0f606102"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'tagger': {'assigns': ['token.tag'],\n",
              "   'requires': [],\n",
              "   'scores': ['tag_acc'],\n",
              "   'retokenizes': False},\n",
              "  'parser': {'assigns': ['token.dep',\n",
              "    'token.head',\n",
              "    'token.is_sent_start',\n",
              "    'doc.sents'],\n",
              "   'requires': [],\n",
              "   'scores': ['dep_uas',\n",
              "    'dep_las',\n",
              "    'dep_las_per_type',\n",
              "    'sents_p',\n",
              "    'sents_r',\n",
              "    'sents_f'],\n",
              "   'retokenizes': False},\n",
              "  'attribute_ruler': {'assigns': [],\n",
              "   'requires': [],\n",
              "   'scores': [],\n",
              "   'retokenizes': False},\n",
              "  'lemmatizer': {'assigns': ['token.lemma'],\n",
              "   'requires': [],\n",
              "   'scores': ['lemma_acc'],\n",
              "   'retokenizes': False},\n",
              "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
              "   'requires': [],\n",
              "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
              "   'retokenizes': False},\n",
              "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
              "   'requires': [],\n",
              "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
              "   'retokenizes': False}},\n",
              " 'problems': {'tok2vec': [],\n",
              "  'tagger': [],\n",
              "  'parser': [],\n",
              "  'attribute_ruler': [],\n",
              "  'lemmatizer': [],\n",
              "  'ner': [],\n",
              "  'entity_ruler': []},\n",
              " 'attrs': {'token.ent_iob': {'assigns': ['ner', 'entity_ruler'],\n",
              "   'requires': []},\n",
              "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
              "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
              "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
              "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
              "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
              "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
              "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
              "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
              "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []}}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\")\n",
        "patterns = [\n",
        "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
        "            ]\n",
        "ruler.add_patterns(patterns)\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "EGZekEtZqzzj",
        "outputId": "18007611-0c65-49d8-f772-a0e94256c2f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4140c2744e20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mruler1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"entity_ruler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m patterns = [\n\u001b[1;32m      3\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"GPE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pattern\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Treblinka\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             ]\n\u001b[1;32m      5\u001b[0m \u001b[0mruler1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m         \u001b[0;31m# Overriding pipe name in the config is not supported and will be ignored.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"name\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E007] 'entity_ruler' already exists in pipeline. Existing names: ['tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner', 'entity_ruler']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample number (555) 555-5555.\"\n",
        "nlpb = spacy.blank(\"en\")\n",
        "ruler = nlpb.add_pipe(\"entity_ruler\")\n",
        "\n",
        "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
        "patterns = [\n",
        "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
        "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]}\n",
        "            ]\n",
        "ruler.add_patterns(patterns)\n",
        "doc = nlpb(text)\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "1wWb2ieGrBFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy Matcher"
      ],
      "metadata": {
        "id": "J4eutOqosrby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "id": "2-sjxXvzrQ5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"LIKE_EMAIL\": True}]\n",
        "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
        "doc = nlp(\"This is an email address: wmattingly@aol.com\")\n",
        "matches = matcher(doc)\n",
        "matches"
      ],
      "metadata": {
        "id": "ljsWQrFJsylQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (nlp.vocab[matches[0][0]].text)"
      ],
      "metadata": {
        "id": "Eg2lTkLutKRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attributes Taken by Matcher\n",
        "* ORTH - The exact verbatim of a token (str)\n",
        "* TEXT - The exact verbatim of a token (str)\n",
        "* LOWER - The lowercase form of the token text  (str)\n",
        "* LENGTH - The length of the token text (int)\n",
        "* IS_ALPHA\n",
        "* IS_ASCII\n",
        "* IS_DIGIT\n",
        "* IS_LOWER\n",
        "* IS_UPPER\n",
        "* IS_TITLE\n",
        "* IS_PUNCT\n",
        "* IS_SPACE\n",
        "* IS_STOP\n",
        "* IS_SENT_START\n",
        "* LIKE_NUM\n",
        "* LIKE_URL\n",
        "* LIKE_EMAIL\n",
        "* SPACY\n",
        "* POS\n",
        "* TAG\n",
        "* MORPH\n",
        "* DEP\n",
        "* LEMMA\n",
        "* SHAPE\n",
        "* ENT_TYPE\n",
        "* _ - Custom extension attributes (Dict[str, Any])\n",
        "* O"
      ],
      "metadata": {
        "id": "Vv7zeNultXSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”. \n",
        "\n",
        "Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think. \n",
        "\n",
        "AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems. \n",
        "\n",
        "While exploiting the power of the computer systems, the curiosity of human, lead him to wonder, “Can a machine think and behave like humans do?” Thus, the development of AI started with the intention of creating similar intelligence in machines that we find and regard high in humans. \n",
        "\n",
        "To Create Expert Systems − The systems which exhibit intelligent behavior, learn, demonstrate, explain, and advice its users. \n",
        "\n",
        "To Implement Human Intelligence in Machines − Creating systems that understand, think, learn, and behave like humans. \n",
        "\n",
        "Artificial intelligence is a science and technology based on disciplines such as Computer Science, Biology, Psychology, Linguistics, Mathematics, and Engineering. \n",
        "\n",
        "A major thrust of AI is in the development of computer functions associated with human intelligence, such as reasoning, learning, and problem solving. \n",
        "\n",
        "Out of the following areas, one or multiple areas can contribute to build an intelligent system. \"\"\"\n",
        "text"
      ],
      "metadata": {
        "id": "XIXms3RGtXCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"POS\": \"PROPN\"}]\n",
        "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print (len(matches))\n",
        "for match in matches[:10]:\n",
        "    print (match, doc[match[1]:match[2]])"
      ],
      "metadata": {
        "id": "cPXsF1UJtPM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1sKPXPp5uEc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}