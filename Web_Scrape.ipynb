{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO56CgqbR1ktFed3WnlQ3LA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VDeshmukhCemtrex/Natural-Language-Processing/blob/main/Web_Scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Jkx26mJMNm",
        "outputId": "ae6595d0-10ba-4c9b-92d2-ab82050e60b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1Processing pipelines\n",
            "2What happens when you call nlp?\n",
            "3Inspecting the pipeline\n",
            "4Custom pipeline components\n",
            "5Use cases for custom components\n",
            "6Simple components\n",
            "7Complex components\n",
            "8Extension attributes\n",
            "9Setting extension attributes (1)\n",
            "10Setting extension attributes (2)\n",
            "11Entities and extensions\n",
            "12Components with extensions\n",
            "13Scaling and performance\n",
            "14Processing streams\n",
            "15Processing data with context\n",
            "16Selective processing\n",
            "« Previous ChapterNext Chapter »\n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# declare URL\n",
        "url = 'https://course.spacy.io/en/chapter3'\n",
        "\n",
        "# make a request and store the response\n",
        "response = requests.get(url)\n",
        "\n",
        "# parse the response using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# locate the element with tag \"div\" and class \"ed518a87\"\n",
        "div = soup.find('div', {'class': 'ed518a87'})\n",
        "\n",
        "# scrape all the \"section\" elements inside\n",
        "sections = div.find_all('section')\n",
        "\n",
        "# print the text attribute\n",
        "for section in sections:\n",
        "  print(section.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Make a request to the website\n",
        "url = \"https://course.spacy.io/en/chapter2\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Locate the \"div\" element with class \"ed518a87\"\n",
        "div_element = soup.find(\"div\", class_=\"ed518a87\")\n",
        "\n",
        "# Scrape all the \"section\" elements with class \"fc31f7f2\" inside the div element\n",
        "sections = div_element.find_all(\"section\", class_=\"fc31f7f2\")\n",
        "\n",
        "# Iterate through the sections and print the text\n",
        "for section in sections:\n",
        "    print(section.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FItFgvuuL8f_",
        "outputId": "955dd70a-8b45-430b-bdbf-d95dd5d34518"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1Data Structures (1)\n",
            "2Strings to hashes\n",
            "3Vocab, hashes and lexemes\n",
            "4Data Structures (2)\n",
            "5Creating a Doc\n",
            "6Docs, spans and entities from scratch\n",
            "7Data structures best practices\n",
            "8Word vectors and semantic similarity\n",
            "9Inspecting word vectors\n",
            "10Comparing similarities\n",
            "11Combining predictions and rules\n",
            "12Debugging patterns (1)\n",
            "13Debugging patterns (2)\n",
            "14Efficient phrase matching\n",
            "15Extracting countries and relationships\n"
          ]
        }
      ]
    }
  ]
}